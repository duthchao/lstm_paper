{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from __future__ import print_function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from datetime import datetime\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "\n",
    "class DFtransformer(object):\n",
    "    def __init__(self,min_df=5):\n",
    "        self.min_df = min_df\n",
    "        self.Vocab = defaultdict(int)\n",
    "        self.Vocab_ = {}\n",
    "    def fit(self,sentence_lst):\n",
    "        for sentence in sentence_lst:\n",
    "            for word in sentence.split():\n",
    "                self.Vocab[word] += 1\n",
    "        print (\"origenal vocab:\",len(self.Vocab))\n",
    "        self.Vocab = {k:v for k,v in self.Vocab.items() if self.Vocab[k]>=self.min_df}   \n",
    "        i = 1\n",
    "        for k in self.Vocab:\n",
    "            self.Vocab_[k] = i\n",
    "            i += 1\n",
    "        print (\"transformed vocab:\",len(self.Vocab))\n",
    "        \n",
    "    def tranform(self,sentence_lst):\n",
    "        ret_s_lst = []\n",
    "        for s in sentence_lst:\n",
    "            ret_s = np.array([self.Vocab_[w] for w in s.split() if w in self.Vocab_])\n",
    "            #print ret_s\n",
    "            ret_s_lst.append(ret_s)\n",
    "        return np.array(ret_s_lst)\n",
    "\n",
    "toker = TreebankWordTokenizer()\n",
    "lemmer = wordnet.WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stop_w = set(stopwords.words('english'))\n",
    "# w2v_model =  '../../../../media/lhc/B2DA42D1DA429191/word2vec_data/GoogleNews-vectors-negative300.bin'\n",
    "# embedder = Word2Vec.load_word2vec_format(w2v_model,binary=True)\n",
    "\n",
    "def str_lem(s):\n",
    "    s = s.lower()\n",
    "    s = (\" \").join([z for z in re.findall('\\w{2,}',s) if z not in stop_w])\n",
    "    s = (\" \").join([lemmer.lemmatize(z) for z in s.split(\" \")])\n",
    "    return s\n",
    "def str_stem(s):\n",
    "    s = s.lower()\n",
    "    s = (\" \").join([z for z in re.findall('\\w{2,}',s) if z not in stop_w])\n",
    "    s = (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n",
    "    return s\n",
    "def apply_text_preprocessor(columns,str_process):\n",
    "    rets = []\n",
    "    for i,row in enumerate(columns,start=1):\n",
    "#         if i%10000==0:\n",
    "#             print i,\n",
    "        rets.append(str_process(row))\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>ticker</th>\n",
       "      <th>title_stem</th>\n",
       "      <th>title_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>0.00272</td>\n",
       "      <td>-0.050633</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>Alcoa (AA) to Shutter Pocos de Caldas Smelter ...</td>\n",
       "      <td>AA</td>\n",
       "      <td>alcoa aa shutter poco de calda smelter brazil ...</td>\n",
       "      <td>alcoa aa shutter pocos de caldas smelter brazi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>0.00272</td>\n",
       "      <td>-0.050633</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>Q2 Earnings Season in the Spotlight with Alcoa...</td>\n",
       "      <td>AA</td>\n",
       "      <td>q2 earn season spotlight alcoa report earn pre...</td>\n",
       "      <td>q2 earnings season spotlight alcoa report earn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>0.00272</td>\n",
       "      <td>-0.050633</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>Earnings will be a nail biter: Jack Ablin</td>\n",
       "      <td>AA</td>\n",
       "      <td>earn nail biter jack ablin</td>\n",
       "      <td>earnings nail biter jack ablin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>0.00272</td>\n",
       "      <td>-0.050633</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>3 Beaten-Down Stocks That May Pay Off for Cont...</td>\n",
       "      <td>AA</td>\n",
       "      <td>beaten stock may pay contrarian investor</td>\n",
       "      <td>beaten stock may pay contrarian investor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-07-03</td>\n",
       "      <td>UPDATE 1-Japan Q3 aluminium premiums mostly se...</td>\n",
       "      <td>AA</td>\n",
       "      <td>updat japan q3 aluminium premium mostli set ye...</td>\n",
       "      <td>update japan q3 aluminium premium mostly set y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        d1       d2        d3        date  \\\n",
       "0  2015-07-02 -0.006306  0.00272 -0.050633  2015-07-02   \n",
       "1  2015-07-02 -0.006306  0.00272 -0.050633  2015-07-02   \n",
       "2  2015-07-02 -0.006306  0.00272 -0.050633  2015-07-02   \n",
       "3  2015-07-02 -0.006306  0.00272 -0.050633  2015-07-02   \n",
       "4         NaN       NaN      NaN       NaN  2015-07-03   \n",
       "\n",
       "                                               title ticker  \\\n",
       "0  Alcoa (AA) to Shutter Pocos de Caldas Smelter ...     AA   \n",
       "1  Q2 Earnings Season in the Spotlight with Alcoa...     AA   \n",
       "2          Earnings will be a nail biter: Jack Ablin     AA   \n",
       "3  3 Beaten-Down Stocks That May Pay Off for Cont...     AA   \n",
       "4  UPDATE 1-Japan Q3 aluminium premiums mostly se...     AA   \n",
       "\n",
       "                                          title_stem  \\\n",
       "0  alcoa aa shutter poco de calda smelter brazil ...   \n",
       "1  q2 earn season spotlight alcoa report earn pre...   \n",
       "2                         earn nail biter jack ablin   \n",
       "3           beaten stock may pay contrarian investor   \n",
       "4  updat japan q3 aluminium premium mostli set ye...   \n",
       "\n",
       "                                           title_lem  \n",
       "0  alcoa aa shutter pocos de caldas smelter brazi...  \n",
       "1  q2 earnings season spotlight alcoa report earn...  \n",
       "2                     earnings nail biter jack ablin  \n",
       "3           beaten stock may pay contrarian investor  \n",
       "4  update japan q3 aluminium premium mostly set y...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin = './data/news_o2.csv'\n",
    "df_all = pd.read_csv(fin,encoding='utf8')\n",
    "#df_all = df_all[df_all.target.notnull()]\n",
    "df_all['title_stem'] = apply_text_preprocessor(df_all['title'],str_stem)\n",
    "df_all['title_lem'] = apply_text_preprocessor(df_all['title'],str_lem)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origenal vocab: 24651\n",
      "transformed vocab: 9290\n"
     ]
    }
   ],
   "source": [
    "target = 'd1'\n",
    "\n",
    "df = df_all[df_all[target].notnull()]\n",
    "dftransformer =  DFtransformer(min_df=5)\n",
    "dftransformer.fit(df['title_lem'])\n",
    "seq_lst = dftransformer.tranform(df['title_lem'])\n",
    "maxlen = 20\n",
    "X = sequence.pad_sequences(seq_lst,maxlen=maxlen)\n",
    "y = (df[target].values>0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 9290 + 1\n",
    "batch_size = 32\n",
    "\n",
    "for tr,va in StratifiedShuffleSplit(y,train_size=0.8,test_size=0.2,random_state=1024,n_iter=1):\n",
    "    X_train = X[tr]\n",
    "    y_train = y[tr]\n",
    "    X_test = X[va]\n",
    "    y_test = y[va]\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('X_train[0]:',X_train[0])\n",
    "print('y_train[0]:',y_train[0])\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# for n in range(15):\n",
    "#     model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1,\n",
    "#               validation_data=(X_test, y_test),verbose=1)\n",
    "#     score, acc = model.evaluate(X_train, y_train,verbose=1,\n",
    "#                                 batch_size=batch_size)\n",
    "#     print()\n",
    "#     print('iter:',n+1)\n",
    "#     print('Train score:', score)\n",
    "#     print('Train accuracy:', acc)\n",
    "#     score, acc = model.evaluate(X_test, y_test,verbose=1,\n",
    "#                                 batch_size=batch_size)\n",
    "#     print('Test score:', score)\n",
    "#     print('Test accuracy:', acc)\n",
    "    \n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=15,\n",
    "          validation_data=(X_test, y_test),verbose=1)\n",
    "score, acc = model.evaluate(X_train, y_train,verbose=1,\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origenal vocab: 24651\n",
      "transformed vocab: 9290\n",
      "X_train shape: (86389L, 30L)\n",
      "X_test shape: (21598L, 30L)\n",
      "X_train[0]: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  425 2571 5021 1323 3524]\n",
      "y_train[0]: 0\n",
      "Build model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_6 (Embedding)          (None, 30, 50)        464550      embedding_input_6[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_6 (Convolution1D)  (None, 28, 250)       37750       embedding_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)                (None, 250)           0           convolution1d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 250)           62750       lambda_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 250)           0           dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 250)           0           dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             251         activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 1)             0           dense_10[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 565301\n",
      "____________________________________________________________________________________________________\n",
      "Train on 86389 samples, validate on 21598 samples\n",
      "Epoch 1/2\n",
      "86389/86389 [==============================] - 1087s - loss: 0.6874 - acc: 0.5294 - val_loss: 0.6828 - val_acc: 0.5509\n",
      "Epoch 2/2\n",
      "86389/86389 [==============================] - 1088s - loss: nan - acc: 0.5106 - val_loss: nan - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x292f5710>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'd1'\n",
    "\n",
    "# set parameters:\n",
    "max_features = 9290 + 1\n",
    "maxlen = 30\n",
    "embedding_dims = 50\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "pool_length = 2\n",
    "hidden_dims = 250\n",
    "\n",
    "batch_size = 32\n",
    "nb_epoch = 2\n",
    "\n",
    "df = df_all[df_all[target].notnull()]\n",
    "dftransformer =  DFtransformer(min_df=5)\n",
    "dftransformer.fit(df['title_lem'])\n",
    "seq_lst = dftransformer.tranform(df['title_lem'])\n",
    "X = sequence.pad_sequences(seq_lst,maxlen=maxlen)\n",
    "y = (df[target].values>0).astype(np.int64)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "for tr,va in StratifiedShuffleSplit(y,train_size=0.8,test_size=0.2,random_state=1024,n_iter=1):\n",
    "    X_train = X[tr]\n",
    "    y_train = y[tr]\n",
    "    X_test = X[va]\n",
    "    y_test = y[va]\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('X_train[0]:',X_train[0])\n",
    "print('y_train[0]:',y_train[0])\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen,\n",
    "                    dropout=0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "\n",
    "# we use max over time pooling by defining a python function to use\n",
    "# in a Lambda layer\n",
    "def max_1d(X):\n",
    "     return K.max(X, axis=1)\n",
    "\n",
    "model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n",
    "#model.add(MaxPooling1D(pool_length=pool_length))\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = 'd1'\n",
    "\n",
    "# set parameters:\n",
    "max_features = 9290 + 1\n",
    "maxlen = 30\n",
    "embedding_dims = 50\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "pool_length = 2\n",
    "hidden_dims = 250\n",
    "\n",
    "batch_size = 320\n",
    "nb_epoch = 2\n",
    "\n",
    "df = df_all[df_all[target].notnull()]\n",
    "dftransformer =  DFtransformer(min_df=5)\n",
    "dftransformer.fit(df['title_lem'])\n",
    "seq_lst = dftransformer.tranform(df['title_lem'])\n",
    "X = sequence.pad_sequences(seq_lst,maxlen=maxlen)\n",
    "y = (df[target].values>0).astype(np.int64)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "for tr,va in StratifiedShuffleSplit(y,train_size=0.8,test_size=0.2,random_state=1024,n_iter=1):\n",
    "    X_train = X[tr]\n",
    "    y_train = y[tr]\n",
    "    X_test = X[va]\n",
    "    y_test = y[va]\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('X_train[0]:',X_train[0])\n",
    "print('y_train[0]:',y_train[0])\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen,\n",
    "                    dropout=0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "\n",
    "# we use max over time pooling by defining a python function to use\n",
    "# in a Lambda layer\n",
    "def max_1d(X):\n",
    "     return K.max(X, axis=1)\n",
    "\n",
    "model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n",
    "#model.add(MaxPooling1D(pool_length=pool_length))\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origenal vocab: 24651\n",
      "transformed vocab: 9290\n",
      "X_train shape: (86389L, 30L)\n",
      "X_test shape: (21598L, 30L)\n",
      "X_train[0]: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  425 2571 5021 1323 3524]\n",
      "y_train[0]: 0\n",
      "Build model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_11 (Embedding)         (None, 30, 128)       1189248     embedding_input_11[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 30, 128)       0           embedding_11[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_11 (Convolution1D) (None, 28, 64)        24640       dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_2 (MaxPooling1D)    (None, 14, 64)        0           convolution1d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 70)            37800       maxpooling1d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 1)             71          lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, 1)             0           dense_18[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 1251759\n",
      "____________________________________________________________________________________________________\n",
      "Train...\n",
      "Train on 86389 samples, validate on 21598 samples\n",
      "Epoch 1/2\n",
      "86389/86389 [==============================] - 783s - loss: 0.6857 - acc: 0.5336 - val_loss: 0.6796 - val_acc: 0.5554\n",
      "Epoch 2/2\n",
      "86389/86389 [==============================] - 778s - loss: 0.6442 - acc: 0.6190 - val_loss: 0.6867 - val_acc: 0.5673\n",
      "21598/21598 [==============================] - 54s    \n",
      "Test score: 0.686706117865\n",
      "Test accuracy: 0.567321048245\n"
     ]
    }
   ],
   "source": [
    "target = 'd1'\n",
    "\n",
    "# Embedding\n",
    "max_features = 9290 + 1\n",
    "maxlen = 30\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "filter_length = 3\n",
    "nb_filter = 64\n",
    "pool_length = 2\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "df = df_all[df_all[target].notnull()]\n",
    "dftransformer =  DFtransformer(min_df=5)\n",
    "dftransformer.fit(df['title_lem'])\n",
    "seq_lst = dftransformer.tranform(df['title_lem'])\n",
    "X = sequence.pad_sequences(seq_lst,maxlen=maxlen)\n",
    "y = (df[target].values>0).astype(np.int64)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "for tr,va in StratifiedShuffleSplit(y,train_size=0.8,test_size=0.2,random_state=1024,n_iter=1):\n",
    "    X_train = X[tr]\n",
    "    y_train = y[tr]\n",
    "    X_test = X[va]\n",
    "    y_test = y[va]\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('X_train[0]:',X_train[0])\n",
    "print('y_train[0]:',y_train[0])\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "model.add(MaxPooling1D(pool_length=pool_length))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, y_test))\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origenal vocab: 24651\n",
      "transformed vocab: 9290\n",
      "X_train shape: (86389L, 20L)\n",
      "X_test shape: (21598L, 20L)\n",
      "X_train[0]: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  425 2571 5021 1323 3524]\n",
      "y_train[0]: 0\n",
      "Build model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_13 (Embedding)         (None, 20, 64)        594624      embedding_input_13[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 20, 64)        0           embedding_13[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_13 (Convolution1D) (None, 18, 64)        12352       dropout_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_4 (MaxPooling1D)    (None, 9, 64)         0           convolution1d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 70)            37800       maxpooling1d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 1)             71          lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, 1)             0           dense_20[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 644847\n",
      "____________________________________________________________________________________________________\n",
      "Train...\n",
      "Train on 86389 samples, validate on 21598 samples\n",
      "Epoch 1/15\n",
      "86389/86389 [==============================] - 279s - loss: 0.6856 - acc: 0.5346 - val_loss: 0.6804 - val_acc: 0.5546\n",
      "Epoch 2/15\n",
      "86389/86389 [==============================] - 287s - loss: 0.6452 - acc: 0.6169 - val_loss: 0.6856 - val_acc: 0.5624\n",
      "Epoch 3/15\n",
      "86389/86389 [==============================] - 287s - loss: 0.5808 - acc: 0.6829 - val_loss: 0.7093 - val_acc: 0.5715\n",
      "Epoch 4/15\n",
      "86389/86389 [==============================] - 287s - loss: 0.5215 - acc: 0.7281 - val_loss: 0.7529 - val_acc: 0.5790\n",
      "Epoch 5/15\n",
      "86389/86389 [==============================] - 286s - loss: 0.4729 - acc: 0.7621 - val_loss: 0.7958 - val_acc: 0.5811\n",
      "Epoch 6/15\n",
      "86389/86389 [==============================] - 288s - loss: 0.4327 - acc: 0.7862 - val_loss: 0.8765 - val_acc: 0.5809\n",
      "Epoch 7/15\n",
      "86389/86389 [==============================] - 285s - loss: 0.4041 - acc: 0.8039 - val_loss: 0.8980 - val_acc: 0.5833\n",
      "Epoch 8/15\n",
      "86389/86389 [==============================] - 144709s - loss: 0.3829 - acc: 0.8136 - val_loss: 0.9630 - val_acc: 0.5841\n",
      "Epoch 9/15\n",
      "86389/86389 [==============================] - 304s - loss: 0.3627 - acc: 0.8256 - val_loss: 1.0256 - val_acc: 0.5846\n",
      "Epoch 10/15\n",
      "86389/86389 [==============================] - 301s - loss: 0.3481 - acc: 0.8332 - val_loss: 1.0471 - val_acc: 0.5826\n",
      "Epoch 11/15\n",
      "86389/86389 [==============================] - 303s - loss: 0.3366 - acc: 0.8371 - val_loss: 1.0748 - val_acc: 0.5849\n",
      "Epoch 12/15\n",
      "86389/86389 [==============================] - 293s - loss: 0.3228 - acc: 0.8450 - val_loss: 1.1318 - val_acc: 0.5858\n",
      "Epoch 13/15\n",
      "86389/86389 [==============================] - 299s - loss: 0.3154 - acc: 0.8507 - val_loss: 1.1500 - val_acc: 0.5859\n",
      "Epoch 14/15\n",
      "86389/86389 [==============================] - 302s - loss: 0.3055 - acc: 0.8550 - val_loss: 1.1368 - val_acc: 0.5880\n",
      "Epoch 15/15\n",
      "86389/86389 [==============================] - 300s - loss: 0.2996 - acc: 0.8573 - val_loss: 1.1953 - val_acc: 0.5866\n",
      "21598/21598 [==============================] - 19s    \n",
      "Test score: 1.19532030913\n",
      "Test accuracy: 0.586628391556\n"
     ]
    }
   ],
   "source": [
    "target = 'd1'\n",
    "\n",
    "# Embedding\n",
    "max_features = 9290 + 1\n",
    "maxlen = 20\n",
    "embedding_size = 64\n",
    "\n",
    "# Convolution\n",
    "filter_length = 3\n",
    "nb_filter = 64\n",
    "pool_length = 2\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "nb_epoch = 15\n",
    "\n",
    "df = df_all[df_all[target].notnull()]\n",
    "dftransformer =  DFtransformer(min_df=5)\n",
    "dftransformer.fit(df['title_lem'])\n",
    "seq_lst = dftransformer.tranform(df['title_lem'])\n",
    "X = sequence.pad_sequences(seq_lst,maxlen=maxlen)\n",
    "y = (df[target].values>0).astype(np.int64)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "for tr,va in StratifiedShuffleSplit(y,train_size=0.8,test_size=0.2,random_state=1024,n_iter=1):\n",
    "    X_train = X[tr]\n",
    "    y_train = y[tr]\n",
    "    X_test = X[va]\n",
    "    y_test = y[va]\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('X_train[0]:',X_train[0])\n",
    "print('y_train[0]:',y_train[0])\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "model.add(MaxPooling1D(pool_length=pool_length))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, y_test))\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86389 samples, validate on 21598 samples\n",
      "Epoch 1/15\n",
      "86389/86389 [==============================] - 292s - loss: 0.2929 - acc: 0.8602 - val_loss: 1.2115 - val_acc: 0.5901\n",
      "Epoch 2/15\n",
      "86389/86389 [==============================] - 289s - loss: 0.2872 - acc: 0.8631 - val_loss: 1.2841 - val_acc: 0.5894\n",
      "Epoch 3/15\n",
      "86389/86389 [==============================] - 293s - loss: 0.2835 - acc: 0.8653 - val_loss: 1.2677 - val_acc: 0.5899\n",
      "Epoch 4/15\n",
      "86389/86389 [==============================] - 297s - loss: 0.2773 - acc: 0.8677 - val_loss: 1.3041 - val_acc: 0.5875\n",
      "Epoch 5/15\n",
      "86389/86389 [==============================] - 294s - loss: 0.2743 - acc: 0.8693 - val_loss: 1.3268 - val_acc: 0.5849\n",
      "Epoch 6/15\n",
      "86389/86389 [==============================] - 303s - loss: 0.2712 - acc: 0.8715 - val_loss: 1.3744 - val_acc: 0.5903\n",
      "Epoch 7/15\n",
      "86389/86389 [==============================] - 311s - loss: 0.2672 - acc: 0.8740 - val_loss: 1.3184 - val_acc: 0.5890\n",
      "Epoch 8/15\n",
      "86389/86389 [==============================] - 310s - loss: 0.2631 - acc: 0.8765 - val_loss: 1.3487 - val_acc: 0.5861\n",
      "Epoch 9/15\n",
      "86389/86389 [==============================] - 299s - loss: 0.2612 - acc: 0.8763 - val_loss: 1.4088 - val_acc: 0.5879\n",
      "Epoch 10/15\n",
      "86389/86389 [==============================] - 296s - loss: 0.2565 - acc: 0.8793 - val_loss: 1.3957 - val_acc: 0.5845\n",
      "Epoch 11/15\n",
      "86389/86389 [==============================] - 297s - loss: 0.2545 - acc: 0.8795 - val_loss: 1.3227 - val_acc: 0.5877\n",
      "Epoch 12/15\n",
      "86389/86389 [==============================] - 301s - loss: 0.2520 - acc: 0.8803 - val_loss: 1.3728 - val_acc: 0.5887\n",
      "Epoch 13/15\n",
      "86389/86389 [==============================] - 293s - loss: 0.2506 - acc: 0.8822 - val_loss: 1.3941 - val_acc: 0.5888\n",
      "Epoch 14/15\n",
      "86389/86389 [==============================] - 290s - loss: 0.2466 - acc: 0.8833 - val_loss: 1.4223 - val_acc: 0.5870\n",
      "Epoch 15/15\n",
      "86389/86389 [==============================] - 297s - loss: 0.2466 - acc: 0.8834 - val_loss: 1.3915 - val_acc: 0.5900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a9e0be0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86389 samples, validate on 21598 samples\n",
      "Epoch 1/15\n",
      "24544/86389 [=======>......................] - ETA: 199s - loss: 0.2281 - acc: 0.8952"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Apple Becomes the Dow's Worst Performer\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.iloc[4402,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>ticker</th>\n",
       "      <th>title_stem</th>\n",
       "      <th>title_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4061</th>\n",
       "      <td>2016-04-26</td>\n",
       "      <td>-0.062578</td>\n",
       "      <td>-0.030566</td>\n",
       "      <td>-0.011494</td>\n",
       "      <td>2016-04-26</td>\n",
       "      <td>Apple Becomes the Dow's Worst Performer</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>appl becom dow worst perform</td>\n",
       "      <td>apple becomes dow worst performer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        d1        d2        d3        date  \\\n",
       "4061  2016-04-26 -0.062578 -0.030566 -0.011494  2016-04-26   \n",
       "\n",
       "                                        title ticker  \\\n",
       "4061  Apple Becomes the Dow's Worst Performer   AAPL   \n",
       "\n",
       "                        title_stem                          title_lem  \n",
       "4061  appl becom dow worst perform  apple becomes dow worst performer  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title'] == u\"Apple Becomes the Dow's Worst Performer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0, 8796, 6385, 4914, 8178, 7664])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[4061]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107987, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107987L, 20L)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
