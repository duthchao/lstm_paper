{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from __future__ import print_function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "from datetime import datetime\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "np.random.seed(1337) \n",
    "\n",
    "class DFtransformer(object):\n",
    "    def __init__(self,min_df=5):\n",
    "        self.min_df = min_df\n",
    "        self.Vocab = defaultdict(int)\n",
    "        self.Vocab_ = {}\n",
    "    def fit(self,sentence_lst):\n",
    "        for sentence in sentence_lst:\n",
    "            for word in sentence.split():\n",
    "                self.Vocab[word] += 1\n",
    "        print (\"origenal vocab:\",len(self.Vocab))\n",
    "        self.Vocab = {k:v for k,v in self.Vocab.items() if self.Vocab[k]>=self.min_df}   \n",
    "        i = 1\n",
    "        for k in self.Vocab:\n",
    "            self.Vocab_[k] = i\n",
    "            i += 1\n",
    "        print (\"transformed vocab:\",len(self.Vocab))\n",
    "        \n",
    "    def tranform(self,sentence_lst):\n",
    "        ret_s_lst = []\n",
    "        for s in sentence_lst:\n",
    "            ret_s = np.array([self.Vocab_[w] for w in s.split() if w in self.Vocab_])\n",
    "            #print ret_s\n",
    "            ret_s_lst.append(ret_s)\n",
    "        return np.array(ret_s_lst)\n",
    "\n",
    "toker = TreebankWordTokenizer()\n",
    "lemmer = wordnet.WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stop_w = set(stopwords.words('english'))\n",
    "# w2v_model =  '../../../../media/lhc/B2DA42D1DA429191/word2vec_data/GoogleNews-vectors-negative300.bin'\n",
    "# embedder = Word2Vec.load_word2vec_format(w2v_model,binary=True)\n",
    "\n",
    "def str_lem(s):\n",
    "    s = s.lower()\n",
    "    s = (\" \").join([z for z in re.findall('\\w{2,}',s) if z not in stop_w])\n",
    "    s = (\" \").join([lemmer.lemmatize(z) for z in s.split(\" \")])\n",
    "    return s\n",
    "def str_stem(s):\n",
    "    s = s.lower()\n",
    "    s = (\" \").join([z for z in re.findall('\\w{2,}',s) if z not in stop_w])\n",
    "    s = (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n",
    "    return s\n",
    "def apply_text_preprocessor(columns,str_process):\n",
    "    rets = []\n",
    "    for i,row in enumerate(columns,start=1):\n",
    "#         if i%10000==0:\n",
    "#             print i,\n",
    "        rets.append(str_process(row))\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>ticker</th>\n",
       "      <th>title_stem</th>\n",
       "      <th>title_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>0.00272</td>\n",
       "      <td>-0.050633</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>Alcoa (AA) to Shutter Pocos de Caldas Smelter ...</td>\n",
       "      <td>AA</td>\n",
       "      <td>alcoa aa shutter poco de calda smelter brazil ...</td>\n",
       "      <td>alcoa aa shutter pocos de caldas smelter brazi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>0.00272</td>\n",
       "      <td>-0.050633</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>Q2 Earnings Season in the Spotlight with Alcoa...</td>\n",
       "      <td>AA</td>\n",
       "      <td>q2 earn season spotlight alcoa report earn pre...</td>\n",
       "      <td>q2 earnings season spotlight alcoa report earn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>0.00272</td>\n",
       "      <td>-0.050633</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>Earnings will be a nail biter: Jack Ablin</td>\n",
       "      <td>AA</td>\n",
       "      <td>earn nail biter jack ablin</td>\n",
       "      <td>earnings nail biter jack ablin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>0.00272</td>\n",
       "      <td>-0.050633</td>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>3 Beaten-Down Stocks That May Pay Off for Cont...</td>\n",
       "      <td>AA</td>\n",
       "      <td>beaten stock may pay contrarian investor</td>\n",
       "      <td>beaten stock may pay contrarian investor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-07-03</td>\n",
       "      <td>UPDATE 1-Japan Q3 aluminium premiums mostly se...</td>\n",
       "      <td>AA</td>\n",
       "      <td>updat japan q3 aluminium premium mostli set ye...</td>\n",
       "      <td>update japan q3 aluminium premium mostly set y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        d1       d2        d3        date  \\\n",
       "0  2015-07-02 -0.006306  0.00272 -0.050633  2015-07-02   \n",
       "1  2015-07-02 -0.006306  0.00272 -0.050633  2015-07-02   \n",
       "2  2015-07-02 -0.006306  0.00272 -0.050633  2015-07-02   \n",
       "3  2015-07-02 -0.006306  0.00272 -0.050633  2015-07-02   \n",
       "4         NaN       NaN      NaN       NaN  2015-07-03   \n",
       "\n",
       "                                               title ticker  \\\n",
       "0  Alcoa (AA) to Shutter Pocos de Caldas Smelter ...     AA   \n",
       "1  Q2 Earnings Season in the Spotlight with Alcoa...     AA   \n",
       "2          Earnings will be a nail biter: Jack Ablin     AA   \n",
       "3  3 Beaten-Down Stocks That May Pay Off for Cont...     AA   \n",
       "4  UPDATE 1-Japan Q3 aluminium premiums mostly se...     AA   \n",
       "\n",
       "                                          title_stem  \\\n",
       "0  alcoa aa shutter poco de calda smelter brazil ...   \n",
       "1  q2 earn season spotlight alcoa report earn pre...   \n",
       "2                         earn nail biter jack ablin   \n",
       "3           beaten stock may pay contrarian investor   \n",
       "4  updat japan q3 aluminium premium mostli set ye...   \n",
       "\n",
       "                                           title_lem  \n",
       "0  alcoa aa shutter pocos de caldas smelter brazi...  \n",
       "1  q2 earnings season spotlight alcoa report earn...  \n",
       "2                     earnings nail biter jack ablin  \n",
       "3           beaten stock may pay contrarian investor  \n",
       "4  update japan q3 aluminium premium mostly set y...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin = './data/news_o2.csv'\n",
    "df_all = pd.read_csv(fin,encoding='utf8')\n",
    "#df_all = df_all[df_all.target.notnull()]\n",
    "df_all['title_stem'] = apply_text_preprocessor(df_all['title'],str_stem)\n",
    "df_all['title_lem'] = apply_text_preprocessor(df_all['title'],str_lem)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origenal vocab: 24651\n",
      "transformed vocab: 9290\n"
     ]
    }
   ],
   "source": [
    "target = 'd1'\n",
    "\n",
    "df = df_all[df_all[target].notnull()]\n",
    "dftransformer =  DFtransformer(min_df=5)\n",
    "dftransformer.fit(df['title_lem'])\n",
    "seq_lst = dftransformer.tranform(df['title_lem'])\n",
    "maxlen = 20\n",
    "X = sequence.pad_sequences(seq_lst,maxlen=maxlen)\n",
    "y = (df[target].values>0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (86389L, 20L)\n",
      "X_test shape: (21598L, 20L)\n",
      "X_train[0]: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  425 2571 5021 1323 3524]\n",
      "y_train[0]: 0\n",
      "Build model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, 20, 128)       1189248     embedding_input_4[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 128)           131584      embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             129         lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 1)             0           dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1320961\n",
      "____________________________________________________________________________________________________\n",
      "Train on 86389 samples, validate on 21598 samples\n",
      "Epoch 1/15\n",
      "86389/86389 [==============================] - 149s - loss: 0.6877 - acc: 0.5266 - val_loss: 0.6822 - val_acc: 0.5450\n",
      "Epoch 2/15\n",
      "86389/86389 [==============================] - 158s - loss: 0.6631 - acc: 0.5924 - val_loss: 0.6810 - val_acc: 0.5578\n",
      "Epoch 3/15\n",
      "86389/86389 [==============================] - 155s - loss: 0.6338 - acc: 0.6237 - val_loss: 0.6856 - val_acc: 0.5597\n",
      "Epoch 4/15\n",
      "86389/86389 [==============================] - 163s - loss: 0.6072 - acc: 0.6490 - val_loss: 0.7034 - val_acc: 0.5640\n",
      "Epoch 5/15\n",
      "86389/86389 [==============================] - 163s - loss: 0.5821 - acc: 0.6691 - val_loss: 0.7077 - val_acc: 0.5638\n",
      "Epoch 6/15\n",
      "86389/86389 [==============================] - 165s - loss: 0.5641 - acc: 0.6839 - val_loss: 0.7307 - val_acc: 0.5677\n",
      "Epoch 7/15\n",
      "86389/86389 [==============================] - 164s - loss: 0.5425 - acc: 0.6988 - val_loss: 0.7409 - val_acc: 0.5709\n",
      "Epoch 8/15\n",
      "86389/86389 [==============================] - 161s - loss: 0.5245 - acc: 0.7144 - val_loss: 0.7735 - val_acc: 0.5722\n",
      "Epoch 9/15\n",
      "86389/86389 [==============================] - 157s - loss: 0.5125 - acc: 0.7216 - val_loss: 0.7860 - val_acc: 0.5758\n",
      "Epoch 10/15\n",
      "86389/86389 [==============================] - 158s - loss: 0.4957 - acc: 0.7349 - val_loss: 0.8126 - val_acc: 0.5785\n",
      "Epoch 11/15\n",
      "86389/86389 [==============================] - 157s - loss: 0.4845 - acc: 0.7432 - val_loss: 0.8376 - val_acc: 0.5747\n",
      "Epoch 12/15\n",
      "86389/86389 [==============================] - 155s - loss: 0.4709 - acc: 0.7509 - val_loss: 0.8585 - val_acc: 0.5768\n",
      "Epoch 13/15\n",
      "86389/86389 [==============================] - 157s - loss: 0.4627 - acc: 0.7574 - val_loss: 0.8814 - val_acc: 0.5773\n",
      "Epoch 14/15\n",
      "86389/86389 [==============================] - 158s - loss: 0.4523 - acc: 0.7645 - val_loss: 0.8914 - val_acc: 0.5825\n",
      "Epoch 15/15\n",
      "86389/86389 [==============================] - 155s - loss: 0.4449 - acc: 0.7706 - val_loss: 0.9110 - val_acc: 0.5837\n",
      "86368/86389 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "max_features = 9290 + 1\n",
    "batch_size = 32\n",
    "\n",
    "for tr,va in StratifiedShuffleSplit(y,train_size=0.8,test_size=0.2,random_state=1024,n_iter=1):\n",
    "    X_train = X[tr]\n",
    "    y_train = y[tr]\n",
    "    X_test = X[va]\n",
    "    y_test = y[va]\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('X_train[0]:',X_train[0])\n",
    "print('y_train[0]:',y_train[0])\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# for n in range(15):\n",
    "#     model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1,\n",
    "#               validation_data=(X_test, y_test),verbose=1)\n",
    "#     score, acc = model.evaluate(X_train, y_train,verbose=1,\n",
    "#                                 batch_size=batch_size)\n",
    "#     print()\n",
    "#     print('iter:',n+1)\n",
    "#     print('Train score:', score)\n",
    "#     print('Train accuracy:', acc)\n",
    "#     score, acc = model.evaluate(X_test, y_test,verbose=1,\n",
    "#                                 batch_size=batch_size)\n",
    "#     print('Test score:', score)\n",
    "#     print('Test accuracy:', acc)\n",
    "    \n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=15,\n",
    "          validation_data=(X_test, y_test),verbose=1)\n",
    "score, acc = model.evaluate(X_train, y_train,verbose=1,\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86389 samples, validate on 21598 samples\n",
      "Epoch 1/15\n",
      "86389/86389 [==============================] - 154s - loss: 0.4372 - acc: 0.7748 - val_loss: 0.9278 - val_acc: 0.5826\n",
      "Epoch 2/15\n",
      "86389/86389 [==============================] - 150s - loss: 0.4276 - acc: 0.7816 - val_loss: 0.9182 - val_acc: 0.5847\n",
      "Epoch 3/15\n",
      "86389/86389 [==============================] - 154s - loss: 0.4229 - acc: 0.7839 - val_loss: 0.9446 - val_acc: 0.5896\n",
      "Epoch 4/15\n",
      "86389/86389 [==============================] - 160s - loss: 0.4162 - acc: 0.7899 - val_loss: 0.9845 - val_acc: 0.5846\n",
      "Epoch 5/15\n",
      "86389/86389 [==============================] - 162s - loss: 0.4119 - acc: 0.7913 - val_loss: 0.9842 - val_acc: 0.5860\n",
      "Epoch 6/15\n",
      "86389/86389 [==============================] - 160s - loss: 0.4062 - acc: 0.7943 - val_loss: 1.0254 - val_acc: 0.5824\n",
      "Epoch 7/15\n",
      "86389/86389 [==============================] - 162s - loss: 0.4011 - acc: 0.7975 - val_loss: 1.0340 - val_acc: 0.5853\n",
      "Epoch 8/15\n",
      "86389/86389 [==============================] - 162s - loss: 0.3975 - acc: 0.8019 - val_loss: 1.0346 - val_acc: 0.5866\n",
      "Epoch 9/15\n",
      "86389/86389 [==============================] - 158s - loss: 0.3942 - acc: 0.8028 - val_loss: 1.0261 - val_acc: 0.5887\n",
      "Epoch 10/15\n",
      "86389/86389 [==============================] - 155s - loss: 0.3934 - acc: 0.8025 - val_loss: 1.0415 - val_acc: 0.5892\n",
      "Epoch 11/15\n",
      "86389/86389 [==============================] - 155s - loss: 0.3879 - acc: 0.8057 - val_loss: 1.0241 - val_acc: 0.5881\n",
      "Epoch 12/15\n",
      "86389/86389 [==============================] - 155s - loss: 0.3826 - acc: 0.8110 - val_loss: 1.0670 - val_acc: 0.5876\n",
      "Epoch 13/15\n",
      "86389/86389 [==============================] - 156s - loss: 0.3825 - acc: 0.8079 - val_loss: 1.0586 - val_acc: 0.5893\n",
      "Epoch 14/15\n",
      "86389/86389 [==============================] - 156s - loss: 0.3789 - acc: 0.8125 - val_loss: 1.0738 - val_acc: 0.5898\n",
      "Epoch 15/15\n",
      "86389/86389 [==============================] - 156s - loss: 0.3779 - acc: 0.8123 - val_loss: 1.0690 - val_acc: 0.5892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29eb1ac8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=15,\n",
    "          validation_data=(X_test, y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86389 samples, validate on 21598 samples\n",
      "Epoch 1/15\n",
      "86389/86389 [==============================] - 178s - loss: 0.3757 - acc: 0.8133 - val_loss: 1.0933 - val_acc: 0.5904\n",
      "Epoch 2/15\n",
      "86389/86389 [==============================] - 163s - loss: 0.3696 - acc: 0.8183 - val_loss: 1.1055 - val_acc: 0.5876\n",
      "Epoch 3/15\n",
      "86389/86389 [==============================] - 178s - loss: 0.3668 - acc: 0.8192 - val_loss: 1.1141 - val_acc: 0.5899\n",
      "Epoch 4/15\n",
      "86389/86389 [==============================] - 169s - loss: 0.3690 - acc: 0.8197 - val_loss: 1.0855 - val_acc: 0.5870\n",
      "Epoch 5/15\n",
      "86389/86389 [==============================] - 169s - loss: 0.3665 - acc: 0.8208 - val_loss: 1.1175 - val_acc: 0.5911\n",
      "Epoch 6/15\n",
      "86389/86389 [==============================] - 157s - loss: 0.3640 - acc: 0.8212 - val_loss: 1.1313 - val_acc: 0.5901\n",
      "Epoch 7/15\n",
      "86389/86389 [==============================] - 149s - loss: 0.3609 - acc: 0.8227 - val_loss: 1.1231 - val_acc: 0.5902\n",
      "Epoch 8/15\n",
      "86389/86389 [==============================] - 147s - loss: 0.3610 - acc: 0.8215 - val_loss: 1.1703 - val_acc: 0.5907\n",
      "Epoch 9/15\n",
      "86389/86389 [==============================] - 151s - loss: 0.3591 - acc: 0.8239 - val_loss: 1.1527 - val_acc: 0.5941\n",
      "Epoch 10/15\n",
      "86389/86389 [==============================] - 152s - loss: 0.3584 - acc: 0.8235 - val_loss: 1.1323 - val_acc: 0.5951\n",
      "Epoch 11/15\n",
      "86389/86389 [==============================] - 150s - loss: 0.3596 - acc: 0.8231 - val_loss: 1.1322 - val_acc: 0.5938\n",
      "Epoch 12/15\n",
      "86389/86389 [==============================] - 150s - loss: 0.3537 - acc: 0.8267 - val_loss: 1.1760 - val_acc: 0.5923\n",
      "Epoch 13/15\n",
      "86389/86389 [==============================] - 149s - loss: 0.3566 - acc: 0.8253 - val_loss: 1.1504 - val_acc: 0.5963\n",
      "Epoch 14/15\n",
      "86389/86389 [==============================] - 148s - loss: 0.3531 - acc: 0.8271 - val_loss: 1.1533 - val_acc: 0.5918\n",
      "Epoch 15/15\n",
      "86389/86389 [==============================] - 149s - loss: 0.3529 - acc: 0.8286 - val_loss: 1.1615 - val_acc: 0.5937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x271306d8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=15,\n",
    "          validation_data=(X_test, y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
